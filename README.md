# BERGEN-UP

>   All mighty tool for RAG like âœ¨BERGEN UPâœ¨

[**BERGEN**](https://github.com/naver/bergen?tab=readme-ov-file) (*BEnchmarking Retrieval-augmented GENeration*) is a library designed to benchmark RAG systems with a focus on question-answering (QA) by **NAVER Labs**. It addresses the challenge of inconsistent benchmarking in comparing approaches and understanding the impact of each component in a RAG pipeline. Unlike BERGEN, BERGEN-UP is an end-to-end evaluation pipeline that enhanced focuses on the diversity of RAG pipelines and the functionality of each modules.

## ğŸ¥‘ What is support to `BERGEN-UP`?
- RAG pipeline experiment management using YAML configuration files
- Support to evaluate APIs for each sub-module


## ğŸ’ Key Feature
- **BERGEN-UP Pipeline** 
    - *Chunking*
        - token level
            - recall
            - precision
            - iou
    - *Pre-Retrieval*
        - multi-query
        - decomposition
        - hyde
    - *Retrieval*
        - evaluation level
            - precision@k
            - recall@k  
            - f1@k
            - ndcg@k
            - hit_rate@k
            - mrr
    - *Generation*
        - standard metrics (G-Eval)
            - groundedness
            - answer_relevancy
            - consistency
            - fluency
            - relevancy
        - custom metrics
- **BENCHMARK Pipeline**
    - *Bench-Test*
        - U-NIAH (Needle in the haystack)
            - selective sub-modules
            - JSON config support
        - FunctionChat Bench
            - Korean function calling evaluation
            - LLM-as-Judge methodology
            - Local model support
        <!-- - BEIR
        - ASQA
        - TriviaQA
        - HotpotQA
        - WikiQA
        - NQ -->
<!-- - **Extra Module** for RAG
    - Generate Synthetic Dataset
        - QA (= Question Answering) -->


## ğŸ‘ How to run pipeline?

##### 1. Write your evaluation in `conf/config.yaml`

##### 2. Run only below script
```bash
$ uv run pipeline.py label='__experiments_name__'
```

## ğŸŠ Core points Each Module

<details>
<summary>Chunking Module</summary>

- í•µì‹¬ ê¸°ëŠ¥
    - Token Level í‰ê°€
        - Metric : (https://research.trychroma.com/evaluating-chunking)
            - iou
            - precision
            - recall

- ì‚¬ìš©ë²•
    - `conf/config.yaml`ì˜ `chunking` ì„¹ì…˜ì— ì•„ë˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•œë‹¤.
    ```yaml
    chunking:
        strategies: 
            - question_set_path: "${hydra:runtime.cwd}/data/chunking/question_set/questions_df_chatlogs.csv"
            - corpora_id_paths:
                chatlogs: "${hydra:runtime.cwd}/data/chunking/corpora/chatlogs.md"
            - Semantic Chunking:
                mode: openai
                embedding_model: "text-embedding-3-large"
                custom_url: "custom_embedding_function_api_address"
            - Recursive Token Chunking:
                chunk_size: 800
                chunk_overlap: 400
            - Fixed Token Chunking:
                chunk_size: 800
                chunk_overlap: 400
    ```

</details>

<details>
<summary>Pre-Retrieval Module</summary>

- í•µì‹¬ ê¸°ëŠ¥
    - LLM-as-a-Judge ê¸°ë°˜ í’ˆì§ˆ í‰ê°€
        - Multi-Query í‰ê°€ ì§€í‘œ:
            - diversity : ìƒì„±ëœ ë‹¤ì¤‘ ì¿¼ë¦¬ë“¤ ê°„ì˜ ë‹¤ì–‘ì„± í‰ê°€ (0-1)
            - coverage : ì›ë³¸ ì¿¼ë¦¬ì˜ ë‹¤ì–‘í•œ ì¸¡ë©´ì„ ì–¼ë§ˆë‚˜ í¬ê´„í•˜ëŠ”ì§€ í‰ê°€ (0-1)
            - relevance : ìƒì„±ëœ ì¿¼ë¦¬ë“¤ì´ ì›ë³¸ ì¿¼ë¦¬ì™€ ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€ í‰ê°€ (0-1)
        - Query Decomposition í‰ê°€ ì§€í‘œ:
            - completeness : ë³µì¡í•œ ì¿¼ë¦¬ë¥¼ ì–¼ë§ˆë‚˜ ì™„ì „í•˜ê²Œ ë¶„í•´í–ˆëŠ”ì§€ í‰ê°€ (0-1)
            - granularity : ë¶„í•´ëœ ì¿¼ë¦¬ë“¤ì˜ ì ì ˆí•œ ì„¸ë¶„í™” ì •ë„ í‰ê°€ (0-1)
            - independence : ê° ë¶„í•´ëœ ì¿¼ë¦¬ê°€ ë…ë¦½ì ìœ¼ë¡œ ë‹µë³€ ê°€ëŠ¥í•œì§€ í‰ê°€ (0-1)
            - answerability : ë¶„í•´ëœ ì¿¼ë¦¬ë“¤ì´ ì‹¤ì œë¡œ ë‹µë³€ ê°€ëŠ¥í•œì§€ í‰ê°€ (0-1)
        - HyDE (Hypothetical Document Embeddings) í‰ê°€ ì§€í‘œ:
            - relevance : ìƒì„±ëœ ê°€ìƒ ë¬¸ì„œê°€ ì¿¼ë¦¬ì™€ ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€ í‰ê°€ (0-1)
            - specificity : ìƒì„±ëœ ë¬¸ì„œê°€ ì–¼ë§ˆë‚˜ êµ¬ì²´ì ì´ê³  ìƒì„¸í•œì§€ í‰ê°€ (0-1)
            - factuality : ìƒì„±ëœ ë¬¸ì„œì˜ ì‚¬ì‹¤ì  ì •í™•ì„± í‰ê°€ (0-1)
            - coherence : ìƒì„±ëœ ë¬¸ì„œì˜ ì¼ê´€ì„±ê³¼ ë…¼ë¦¬ì  íë¦„ í‰ê°€ (0-1)

- ì‚¬ìš©ë²•
    - `conf/config.yaml`ì˜ `pre_retrieval` ì„¹ì…˜ì— ì•„ë˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•œë‹¤.
    ```yaml
    pre_retrieval:
        strategies: 
            - Multi Query:
                path: "${hydra:runtime.cwd}/data/pre_retrieval/multi_query/sample_data.json"
            - Query Decomposition:
                path: "${hydra:runtime.cwd}/data/pre_retrieval/query_decomposition/sample_data.json"
            - HyDE:
                path: "${hydra:runtime.cwd}/data/pre_retrieval/hyde/sample_data.json"
    ```

</details>

<details>
<summary>Retrieval Module</summary>

- í•µì‹¬ ê¸°ëŠ¥
    - Evaluation Level í‰ê°€
        - Metric : 
            - precision@k : ê²€ìƒ‰ëœ ìƒìœ„ kê°œ ê²°ê³¼ ì¤‘ ê´€ë ¨ ë¬¸ì„œì˜ ë¹„ìœ¨
            - recall@k : ì „ì²´ ê´€ë ¨ ë¬¸ì„œ ì¤‘ ìƒìœ„ kê°œ ê²°ê³¼ì—ì„œ ê²€ìƒ‰ëœ ë¹„ìœ¨
            - f1@k : precision@kì™€ recall@kì˜ ì¡°í™”í‰ê· 
            - ndcg@k : ìˆœìœ„ë¥¼ ê³ ë ¤í•œ ëˆ„ì  í• ì¸ ê²Œì¸
            - hit_rate@k : ìƒìœ„ kê°œ ê²°ê³¼ì— ê´€ë ¨ ë¬¸ì„œê°€ í•˜ë‚˜ë¼ë„ ìˆëŠ” ë¹„ìœ¨
            - mrr : ì²« ë²ˆì§¸ ê´€ë ¨ ë¬¸ì„œì˜ ìˆœìœ„ ì—­ìˆ˜ í‰ê· 

- ì‚¬ìš©ë²•
    - `conf/config.yaml`ì˜ `retrieval` ì„¹ì…˜ì— ì•„ë˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•œë‹¤.
    ```yaml
    retrieval:
        strategies: 
            - sample_data_path: "${hydra:runtime.cwd}/data/retrieval/sample_data.json"
            - top_k: 10
    ```

</details>

<details>
<summary>Generation Module</summary>

- í•µì‹¬ ê¸°ëŠ¥
    - G-Eval ê¸°ë°˜ ìƒì„± í’ˆì§ˆ í‰ê°€
        - Standard Metrics (í‘œì¤€ í‰ê°€ ì§€í‘œ):
            - groundedness : ìƒì„±ëœ ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ì–¼ë§ˆë‚˜ ê·¼ê±°í•˜ëŠ”ì§€ í‰ê°€ (0-1)
            - answer_relevancy : ìƒì„±ëœ ë‹µë³€ì´ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€ í‰ê°€ (0-1)
            - consistency : ìƒì„±ëœ ë‹µë³€ì˜ ë‚´ë¶€ ì¼ê´€ì„± í‰ê°€ (0-1)
            - fluency : ìƒì„±ëœ ë‹µë³€ì˜ ìœ ì°½ì„± ë° ê°€ë…ì„± í‰ê°€ (0-1)
            - relevancy : ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€ í‰ê°€ (0-1)
        - Custom Metrics (ì‚¬ìš©ì ì •ì˜ í‰ê°€ ì§€í‘œ):
            - ì‚¬ìš©ìê°€ ì •ì˜í•œ í‰ê°€ ê¸°ì¤€ì— ë”°ë¥¸ ë§ì¶¤í˜• í‰ê°€ ê°€ëŠ¥
            - 1-5 ì  ì²™ë„ë¡œ ì„¸ë°€í•œ í‰ê°€ ì§€ì›

- ì‚¬ìš©ë²•
    - `conf/config.yaml`ì˜ `generation` ì„¹ì…˜ì— ì•„ë˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•œë‹¤.
    
    **ê¸°ë³¸ ì‚¬ìš©ë²• (Standard Metrics):**
    ```yaml
    generation:
        strategies: 
            - sample_data_path: "${hydra:runtime.cwd}/data/generation/sample_generation_data.json"
            - evaluation_metrics:
                - groundedness
                - answer_relevancy
            - g_eval_config:
                mode: "standard"
                metric_name: "Answer Relevancy"  # ì„ íƒ ê°€ëŠ¥: Answer Relevancy, Consistency, Fluency, Groundness, Relevancy
                metric_llm:
                    model_name: "gpt-4"
                    temperature: 0.0
                    max_tokens: 1024
    ```
    
    **ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ì‚¬ìš©ë²•:**
    ```yaml
    generation:
        strategies: 
            - sample_data_path: "${hydra:runtime.cwd}/data/generation/sample_generation_data.json"
            - g_eval_config:
                mode: "custom"
                metric_name: "Technical Accuracy"
                metric_description: "Evaluating how technically accurate and precise the answer is"
                metric_criterion: |
                    - 1: Very Poor. The answer contains significant technical errors.
                    - 2: Poor. The answer has some technical accuracy but contains notable errors.
                    - 3: Fair. The answer is generally accurate but lacks precision.
                    - 4: Good. The answer is technically accurate with minor issues.
                    - 5: Excellent. The answer is perfectly accurate and technically precise.
                metric_llm:
                    model_name: "gpt-4"
                    temperature: 0.0
                    max_tokens: 1024
    ```

</details>

<details>
<summary>Benchmark Module</summary>

- í•µì‹¬ ê¸°ëŠ¥
    - NIAH (Needle In A Haystack) í‰ê°€
        - ê¸´ ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œ íŠ¹ì • ì •ë³´ë¥¼ ì°¾ëŠ” ëŠ¥ë ¥ í‰ê°€
        - ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì§€ì›:
            - single_needle : ë‹¨ì¼ ì •ë³´ ê²€ìƒ‰
            - multi_needle : ë‹¤ì¤‘ ì •ë³´ ê²€ìƒ‰
            - complex_info : ë³µì¡í•œ ì •ë³´ ê²€ìƒ‰
            - password_test : ì•”í˜¸ ì°¾ê¸° í…ŒìŠ¤íŠ¸
            - location_test : ìœ„ì¹˜ ì •ë³´ ì°¾ê¸° í…ŒìŠ¤íŠ¸
        - ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ê¹Šì´ì— ë”°ë¥¸ ì„±ëŠ¥ ë¶„ì„
        - ì„ íƒì  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰ ì§€ì›
        - JSON/YAML ì„¤ì • íŒŒì¼ ì§€ì›

- ì‚¬ìš©ë²•
    - `conf/config.yaml`ì˜ `benchmark` ì„¹ì…˜ì— ì•„ë˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•œë‹¤.
    
    **ê¸°ë³¸ ì‚¬ìš©ë²•:**
    ```yaml
    benchmark:
        strategies:
            - llm_endpoint: "openai/gpt-4o"
            - needle_config_path: "${hydra:runtime.cwd}/data/benchmark/NIAH/needle_config.json"
            - NIAH:
                context_lengths: [1000, 2000, 4000]
                document_depth_percents: [0.1, 0.5, 0.9]
                num_samples_per_test: 2
                save_results: true
                save_contexts: false
                test_cases: ["single_needle", "multi_needle", "complex_info"]  # ì‹¤í–‰í•  í…ŒìŠ¤íŠ¸ ì„ íƒ
    ```
    
    **needle_config.json í˜•ì‹:**
    ```json
    {
        "single_needle": {
            "needles": ["The secret code is ALPHA-7234."],
            "question": "What is the secret code?",
            "true_answer": "ALPHA-7234"
        },
        "multi_needle": {
            "needles": [
                "The meeting will be held in Conference Room B.",
                "The meeting time is 3:30 PM.",
                "The meeting date is next Tuesday."
            ],
            "question": "When and where is the meeting?",
            "true_answer": "The meeting will be held in Conference Room B at 3:30 PM next Tuesday."
        },
        "complex_info": {
            "needles": [
                "Dr. Smith discovered the rare element Xenium in 2019.",
                "Xenium has atomic number 142.",
                "The element exhibits superconducting properties at room temperature."
            ],
            "question": "What are the key facts about Xenium?",
            "true_answer": "Dr. Smith discovered Xenium in 2019. It has atomic number 142 and exhibits superconducting properties at room temperature."
        }
    }
    ```
    
    **íŠ¹ì • í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰í•˜ê¸°:**
    ```yaml
    benchmark:
        strategies:
            - llm_endpoint: "openai/gpt-4o"
            - needle_config_path: "${hydra:runtime.cwd}/data/benchmark/NIAH/needle_config.json"
            - NIAH:
                context_lengths: [1000, 2000]
                document_depth_percents: [0.1, 0.5]
                num_samples_per_test: 1
                save_results: true
                save_contexts: false
                test_cases: ["single_needle", "multi_needle"]  # 2ê°œ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
    ```

    **FunctionChat Bench í‰ê°€:**
    
    FunctionChat-BenchëŠ” í•œêµ­ì–´ LLMì˜ í•¨ìˆ˜ í˜¸ì¶œ(function calling) ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤.
    
    - ì£¼ìš” íŠ¹ì§•:
        - Dialog/SingleCall ë‘ ê°€ì§€ í‰ê°€ íƒ€ì… ì§€ì›
        - Exact Matchì™€ LLM-as-Judge í‰ê°€ ë°©ì‹
        - OpenAI í˜¸í™˜ API ì§€ì› (ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥)
        - í•œêµ­ì–´ í•¨ìˆ˜ í˜¸ì¶œ ì‹œë‚˜ë¦¬ì˜¤ í‰ê°€
    
    **ê¸°ë³¸ ì‚¬ìš©ë²•:**
    ```yaml
    function_chat:
        strategies:
            # í‰ê°€í•  ëª¨ë¸ ì„¤ì •
            - llm_model_name: "gpt-4o"
            - llm_api_key: "${common.OPENAI_API_KEY}"
            - llm_endpoint: "https://api.openai.com/v1"
            
            # í‰ê°€ì ëª¨ë¸ ì„¤ì • (ì„ íƒì‚¬í•­, ê¸°ë³¸ê°’: GPT-4)
            - evaluator_model: "gpt-4"
            - evaluator_endpoint: "https://api.openai.com/v1"
            
            # í‰ê°€ ì„¤ì •
            - evaluation_types: ["dialog", "singlecall"]  # í‰ê°€ íƒ€ì… ì„ íƒ
            - data_path: "${hydra:runtime.cwd}/data/benchmark/functionchat_bench"
            - dataset_files:  # ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ íŒŒì¼ ì§€ì • (ì„ íƒì‚¬í•­)
                dialog: "FunctionChat-Dialog-Sample.jsonl"
                singlecall: "FunctionChat-Singlecall-Sample.jsonl"
            - temperature: 0.0
            - tool_choice: "auto"
            - only_exact: false  # true: exact matchë§Œ, false: LLM í‰ê°€ í¬í•¨
    ```
    
    **ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ì˜ˆì‹œ:**
    ```yaml
    function_chat:
        strategies:
            # vLLM ë˜ëŠ” ë‹¤ë¥¸ OpenAI í˜¸í™˜ ì„œë²„ ì‚¬ìš©
            - llm_model_name: "llama-3-70b-instruct"
            - llm_api_key: "dummy-key"  # ë¡œì»¬ ì„œë²„ì—ì„œëŠ” ë¬´ì‹œë¨
            - llm_endpoint: "http://localhost:8000/v1"
            
            # í‰ê°€ìëŠ” GPT-4 ì‚¬ìš© (ê¶Œì¥)
            - evaluator_model: "gpt-4"
            - evaluator_endpoint: "https://api.openai.com/v1"
            
            - evaluation_types: ["singlecall"]
            - only_exact: false
    ```
    
    **ë°ì´í„°ì…‹ í˜•ì‹:**
    - Dialog: ë‹¤ì¤‘ í„´ ëŒ€í™”ì—ì„œì˜ í•¨ìˆ˜ í˜¸ì¶œ í‰ê°€
    - SingleCall: ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•œ í•¨ìˆ˜ í˜¸ì¶œ í‰ê°€
    - ê° ì˜ˆì œëŠ” tools, query, ground_truthë¥¼ í¬í•¨
    
    **í‰ê°€ ê²°ê³¼:**
    - Accuracy: ì „ì²´ ì •ë‹µë¥ 
    - ìƒì„¸ ê²°ê³¼ëŠ” `outputs/function_chat_summary.json`ì— ì €ì¥

</details>